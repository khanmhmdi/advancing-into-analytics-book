{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install hdbscan\n!pip install umap-learn\n!pip install squarify","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-09-17T21:19:36.019646Z","iopub.execute_input":"2023-09-17T21:19:36.020736Z","iopub.status.idle":"2023-09-17T21:21:38.230634Z","shell.execute_reply.started":"2023-09-17T21:19:36.020695Z","shell.execute_reply":"2023-09-17T21:21:38.229292Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Libraries","metadata":{}},{"cell_type":"code","source":"import umap\nimport random\nimport hdbscan\nimport squarify  \nimport numpy as np \nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.cm as cm\nimport plotly.express as px\nimport category_encoders as ce\nimport matplotlib.pyplot as plt \n\n\nfrom sklearn.svm import SVC\nfrom sklearn.cluster import KMeans\nfrom scipy.cluster import hierarchy \nfrom sklearn.decomposition import PCA\nfrom scipy.spatial import distance_matrix \nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.cluster import OPTICS, cluster_optics_dbscan\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.metrics import silhouette_score, silhouette_samples\nfrom sklearn.metrics import silhouette_score\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.manifold import TSNE\n\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2023-09-17T21:22:13.556852Z","iopub.execute_input":"2023-09-17T21:22:13.557318Z","iopub.status.idle":"2023-09-17T21:22:45.918088Z","shell.execute_reply.started":"2023-09-17T21:22:13.557278Z","shell.execute_reply":"2023-09-17T21:22:45.917185Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load Data","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/ecommerce-dataset-for-predictive-marketing-2023/ECommerce_consumer behaviour.csv')","metadata":{"execution":{"iopub.status.busy":"2023-09-17T21:25:17.455315Z","iopub.execute_input":"2023-09-17T21:25:17.455755Z","iopub.status.idle":"2023-09-17T21:25:20.423841Z","shell.execute_reply.started":"2023-09-17T21:25:17.455724Z","shell.execute_reply":"2023-09-17T21:25:20.422459Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df","metadata":{"execution":{"iopub.status.busy":"2023-09-17T21:25:20.426319Z","iopub.execute_input":"2023-09-17T21:25:20.426698Z","iopub.status.idle":"2023-09-17T21:25:20.450731Z","shell.execute_reply.started":"2023-09-17T21:25:20.426668Z","shell.execute_reply":"2023-09-17T21:25:20.449392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in df.columns:\n    print(df[i].value_counts())","metadata":{"execution":{"iopub.status.busy":"2023-09-17T21:25:20.452720Z","iopub.execute_input":"2023-09-17T21:25:20.453265Z","iopub.status.idle":"2023-09-17T21:25:21.442291Z","shell.execute_reply.started":"2023-09-17T21:25:20.453221Z","shell.execute_reply":"2023-09-17T21:25:21.440821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.shape","metadata":{"execution":{"iopub.status.busy":"2023-09-17T21:25:21.445918Z","iopub.execute_input":"2023-09-17T21:25:21.446396Z","iopub.status.idle":"2023-09-17T21:25:21.454604Z","shell.execute_reply.started":"2023-09-17T21:25:21.446356Z","shell.execute_reply":"2023-09-17T21:25:21.453251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.describe().T\n# This mean that we have only 9 numerical column ","metadata":{"execution":{"iopub.status.busy":"2023-09-17T21:25:21.456507Z","iopub.execute_input":"2023-09-17T21:25:21.456967Z","iopub.status.idle":"2023-09-17T21:25:22.114502Z","shell.execute_reply.started":"2023-09-17T21:25:21.456926Z","shell.execute_reply":"2023-09-17T21:25:22.113178Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.describe().T\n# This mean that we have only 9 numerical column ","metadata":{"execution":{"iopub.status.busy":"2023-09-17T21:25:22.115898Z","iopub.execute_input":"2023-09-17T21:25:22.116250Z","iopub.status.idle":"2023-09-17T21:25:22.771653Z","shell.execute_reply.started":"2023-09-17T21:25:22.116220Z","shell.execute_reply":"2023-09-17T21:25:22.770520Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.describe().T\n# This mean that we have only 9 numerical column ","metadata":{"execution":{"iopub.status.busy":"2023-09-17T21:25:22.773436Z","iopub.execute_input":"2023-09-17T21:25:22.774359Z","iopub.status.idle":"2023-09-17T21:25:23.416630Z","shell.execute_reply.started":"2023-09-17T21:25:22.774324Z","shell.execute_reply":"2023-09-17T21:25:23.415246Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.nunique()","metadata":{"execution":{"iopub.status.busy":"2023-09-17T21:25:23.418577Z","iopub.execute_input":"2023-09-17T21:25:23.418971Z","iopub.status.idle":"2023-09-17T21:25:23.981943Z","shell.execute_reply.started":"2023-09-17T21:25:23.418931Z","shell.execute_reply":"2023-09-17T21:25:23.980676Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.hist(figsize=(15,10))\nplt.subplots_adjust(hspace=0.5)","metadata":{"execution":{"iopub.status.busy":"2023-09-17T21:25:23.983393Z","iopub.execute_input":"2023-09-17T21:25:23.983711Z","iopub.status.idle":"2023-09-17T21:25:27.017578Z","shell.execute_reply.started":"2023-09-17T21:25:23.983684Z","shell.execute_reply":"2023-09-17T21:25:27.016674Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see many of order numbers are range from 0-10 from each product and the max is 100. many of customers order product in 9 Am- 22 PM of day. most of the customers routin order is between 2-10 days.","metadata":{}},{"cell_type":"code","source":"# Correlation Between Continous & Target\nplt.figure(figsize=(15,10))\nsns.heatmap(df.corr(), annot=True)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-17T21:25:27.021477Z","iopub.execute_input":"2023-09-17T21:25:27.022576Z","iopub.status.idle":"2023-09-17T21:25:28.498574Z","shell.execute_reply.started":"2023-09-17T21:25:27.022539Z","shell.execute_reply":"2023-09-17T21:25:28.497134Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['order_hour_of_day'].value_counts()\n\nplt.figure(figsize = (10,7))\nsns.set_style(\"ticks\")\nsns.countplot(data=df, x=df['order_hour_of_day'], alpha=0.6)\nplt.title(\"Busiest times of the day\", fontsize=14)\nplt.xlabel(\"Hour of the day\", fontsize=12)\nplt.ylabel(\"Number of orders\", fontsize=12)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-17T21:25:28.500029Z","iopub.execute_input":"2023-09-17T21:25:28.500442Z","iopub.status.idle":"2023-09-17T21:25:30.354060Z","shell.execute_reply.started":"2023-09-17T21:25:28.500403Z","shell.execute_reply":"2023-09-17T21:25:30.352609Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"departments = df['department'].value_counts()\n\ndepartments_df = pd.DataFrame(departments).reset_index()\n\ndepartments_df.columns = ['Department', 'Order Count']\n\ntop_departments = departments_df.sort_values(by='Order Count', ascending=False).head()\n\nplt.figure(figsize = (10,7))\nsns.set_style(\"ticks\")\nsns.barplot(data=top_departments, x=\"Order Count\", y=\"Department\", palette = 'flare', alpha=0.6)\nplt.title(\"Top 5 most popular departments\", fontsize=14)\nplt.xlabel(\"Number of orders\", fontsize=12)\nplt.ylabel(\"Department name\", fontsize=12)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-17T21:25:30.355833Z","iopub.execute_input":"2023-09-17T21:25:30.356273Z","iopub.status.idle":"2023-09-17T21:25:30.952432Z","shell.execute_reply.started":"2023-09-17T21:25:30.356234Z","shell.execute_reply":"2023-09-17T21:25:30.951084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"products = df['product_name'].value_counts()\n\nproducts_df = pd.DataFrame(products).reset_index()\n\nproducts_df.columns = ['Product', 'Order Count']\n\ntop_products = products_df.sort_values(by='Order Count', ascending=False).head()\nplt.figure(figsize = (10,7))\nsns.set_style(\"ticks\")\nsns.barplot(data=top_products, x=\"Order Count\", y=\"Product\", palette = 'flare', alpha=0.6)\nplt.title(\"Top 5 most popular products\", fontsize=14)\nplt.xlabel(\"Number of orders\", fontsize=12)\nplt.ylabel(\"Product name\", fontsize=12)\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-09-17T21:25:30.954334Z","iopub.execute_input":"2023-09-17T21:25:30.954809Z","iopub.status.idle":"2023-09-17T21:25:31.556665Z","shell.execute_reply.started":"2023-09-17T21:25:30.954740Z","shell.execute_reply":"2023-09-17T21:25:31.555060Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Crosstab to check distribution of products in each department\nproduct_dept_df = pd.crosstab(df['department'], df['product_name'])\nproduct_dept_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-17T21:25:31.558274Z","iopub.execute_input":"2023-09-17T21:25:31.558700Z","iopub.status.idle":"2023-09-17T21:25:32.343578Z","shell.execute_reply.started":"2023-09-17T21:25:31.558669Z","shell.execute_reply":"2023-09-17T21:25:32.342001Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"product_dept_df.idxmax(axis=1).to_frame(name=\"Most popular Item\")","metadata":{"execution":{"iopub.status.busy":"2023-09-17T21:25:32.345387Z","iopub.execute_input":"2023-09-17T21:25:32.346215Z","iopub.status.idle":"2023-09-17T21:25:32.362225Z","shell.execute_reply.started":"2023-09-17T21:25:32.346170Z","shell.execute_reply":"2023-09-17T21:25:32.361101Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"product_reordered_df = df.groupby('product_name')['reordered'].count().reset_index().sort_values(by='reordered', ascending=False)\nproduct_reordered_df\n","metadata":{"execution":{"iopub.status.busy":"2023-09-17T21:25:32.363937Z","iopub.execute_input":"2023-09-17T21:25:32.364874Z","iopub.status.idle":"2023-09-17T21:25:32.604176Z","shell.execute_reply.started":"2023-09-17T21:25:32.364831Z","shell.execute_reply":"2023-09-17T21:25:32.603069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize = (10,7))\nsns.set_style(\"ticks\")\nsns.barplot(data=product_reordered_df.head(5), x=\"reordered\", y=\"product_name\", palette = 'flare', alpha=0.6)\nplt.title(\"Top 5 most reordered products\", fontsize=14)\nplt.xlabel(\"Number of reorders\", fontsize=12)\nplt.ylabel(\"Product name\", fontsize=12)\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-09-17T21:25:32.605971Z","iopub.execute_input":"2023-09-17T21:25:32.606343Z","iopub.status.idle":"2023-09-17T21:25:32.858048Z","shell.execute_reply.started":"2023-09-17T21:25:32.606303Z","shell.execute_reply":"2023-09-17T21:25:32.856875Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# AGGREGATING & GROUPING VALUES TO VISUALIZE PURCHASING BEHAVIOUR\ngrouped = df.groupby(\"order_id\")[\"add_to_cart_order\"].aggregate(\"max\").reset_index()\ngrouped = grouped.add_to_cart_order.value_counts()\n\nsns.set_style('dark')\nsns.set_palette(\"rocket_r\")\nf, ax = plt.subplots(figsize=(15, 12))\nsns.barplot(x=grouped.index, y=grouped.values, ax=ax)\nax.grid(True, axis='y')\n\nplt.xticks(rotation='vertical', fontsize=12)\nplt.yticks(fontsize=12)\nplt.ylabel('Number of unique orders', fontsize=15)\nplt.xlabel('Number of products added to cart', fontsize=15)\nplt.title('Purchasing Behavior by Number of Products Added to Cart', fontsize=18)\nplt.xlim(0, 35)  # limit the X axis values to 35\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-17T21:25:32.859992Z","iopub.execute_input":"2023-09-17T21:25:32.860989Z","iopub.status.idle":"2023-09-17T21:25:34.252421Z","shell.execute_reply.started":"2023-09-17T21:25:32.860946Z","shell.execute_reply":"2023-09-17T21:25:34.250897Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# TIME OF THE DAY WHEN THE ORDER WAS MADE\ngrouped = df.groupby('order_hour_of_day', as_index=True).agg({'user_id':'count'}).sort_values(by='user_id',ascending=False)\n\n\nf, ax = plt.subplots(figsize=(15, 12))\nplt.xticks(rotation='vertical')\nsns.barplot(x = grouped.index, y = grouped.user_id)\nsns.color_palette(\"rocket_r\", 10)\n\nplt.ylabel('Number of unique orders', fontsize=13)\nplt.xlabel('Time of the day', fontsize=13)\n\nfor i in ax.patches:\n    ax.text(i.get_x()+0.15, i.get_height()+50, str(round(i.get_height())), fontsize=10, color='black')\nplt.show()\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-17T21:25:34.254658Z","iopub.execute_input":"2023-09-17T21:25:34.255176Z","iopub.status.idle":"2023-09-17T21:25:34.994951Z","shell.execute_reply.started":"2023-09-17T21:25:34.255134Z","shell.execute_reply":"2023-09-17T21:25:34.993479Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def order_number_group(num_orders):\n    ranges = [(1, 10), (11, 20), (21, 30), (31, 40), (41, 50),(51, 60),(61, 70),(71, 80),(81, 90),(91, 100)]\n    for r in ranges:\n        if num_orders in range(r[0], r[1]+1):\n            return f\"{r[0]}-{r[1]} orders\"\n    return \"More than 100 orders\"","metadata":{"execution":{"iopub.status.busy":"2023-09-17T21:25:34.996644Z","iopub.execute_input":"2023-09-17T21:25:34.997069Z","iopub.status.idle":"2023-09-17T21:25:35.005258Z","shell.execute_reply.started":"2023-09-17T21:25:34.997035Z","shell.execute_reply":"2023-09-17T21:25:35.003979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"top_products = df.groupby('product_name')['user_id'].count().sort_values(ascending=False).head(10)\ncolors = ['#4C72B0', '#55A868', '#C44E52', '#8172B2', '#CCB974', '#64B5CD', '#4E79A7', '#E69F00', '#F0E442', '#59A14F', '#8C8C8C', '#9C755F', '#EDB8A7', '#BDBDBD', '#000000']\nax = top_products.plot(kind='bar', title='Top 10 Products', color=colors, figsize=(15, 15))\nplt.xlabel('Product Name',fontsize=13)\nplt.ylabel('Number of Orders',fontsize=13)\nfor i in ax.patches:\n    ax.text(i.get_x()+0.15, i.get_height()+50, str(round(i.get_height())), fontsize=10, color='black')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-17T21:25:35.007344Z","iopub.execute_input":"2023-09-17T21:25:35.008158Z","iopub.status.idle":"2023-09-17T21:25:35.735819Z","shell.execute_reply.started":"2023-09-17T21:25:35.008108Z","shell.execute_reply":"2023-09-17T21:25:35.734467Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bottom_products = df.groupby('product_name')['user_id'].count().sort_values(ascending=False).tail(10)\ncolors = ['#4C72B0', '#55A868', '#C44E52', '#8172B2', '#CCB974', '#64B5CD', '#4E79A7', '#E69F00', '#F0E442', '#59A14F', '#8C8C8C', '#9C755F', '#EDB8A7', '#BDBDBD', '#000000']\nax = bottom_products.plot(kind='bar', title='Bottom 10 Products', color=colors, figsize=(15, 15))\nplt.xlabel('Product Name',fontsize=13)\nplt.ylabel('Number of Orders',fontsize=13)\nfor i in ax.patches:\n    ax.text(i.get_x()+0.15, i.get_height()+50, str(round(i.get_height())), fontsize=10, color='black')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-17T21:25:35.737577Z","iopub.execute_input":"2023-09-17T21:25:35.737986Z","iopub.status.idle":"2023-09-17T21:25:36.467910Z","shell.execute_reply.started":"2023-09-17T21:25:35.737949Z","shell.execute_reply":"2023-09-17T21:25:36.467050Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # TREEMAP VISUALIZATION OF ITEMS\n# df.dropna(inplace=True)\n# fig, ax = plt.subplots(figsize=(15, 15))\n# fig = squarify.plot(sizes=df['product_name'].value_counts(), label=df['product_name'])\n","metadata":{"execution":{"iopub.status.busy":"2023-09-17T21:25:36.469040Z","iopub.execute_input":"2023-09-17T21:25:36.469535Z","iopub.status.idle":"2023-09-17T21:25:36.473908Z","shell.execute_reply.started":"2023-09-17T21:25:36.469504Z","shell.execute_reply":"2023-09-17T21:25:36.473123Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Cleaning","metadata":{}},{"cell_type":"markdown","source":"## Remove Outlier","metadata":{}},{"cell_type":"code","source":"def remove_rare_categories(df, categorical_columns, threshold):\n    \"\"\"\n    Remove rare categories from categorical columns in a DataFrame.\n    \n    Args:\n        df (pandas.DataFrame): Input DataFrame.\n        categorical_columns (list): List of categorical column names to analyze.\n        threshold (float): Threshold to determine rare categories (e.g., 0.01 for 1% occurrence).\n    \n    Returns:\n        pandas.DataFrame: DataFrame with rare categories removed.\n    \"\"\"\n    df_cleaned = df.copy()\n    for column in categorical_columns:\n        category_counts = df_cleaned[column].value_counts(normalize=True)\n        rare_categories = category_counts[category_counts < threshold].index\n        df_cleaned = df_cleaned[~df_cleaned[column].isin(rare_categories)]\n    \n    return df_cleaned","metadata":{"execution":{"iopub.status.busy":"2023-09-17T21:25:36.481271Z","iopub.execute_input":"2023-09-17T21:25:36.481911Z","iopub.status.idle":"2023-09-17T21:25:36.491979Z","shell.execute_reply.started":"2023-09-17T21:25:36.481865Z","shell.execute_reply":"2023-09-17T21:25:36.490441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"columns = ['order_dow','order_dow','product_id','add_to_cart_order','reordered','department_id','department','product_name']\nthreshold = 0.001\ndf = remove_rare_categories(df,columns,threshold)","metadata":{"execution":{"iopub.status.busy":"2023-09-17T21:25:36.494099Z","iopub.execute_input":"2023-09-17T21:25:36.494639Z","iopub.status.idle":"2023-09-17T21:25:38.876392Z","shell.execute_reply.started":"2023-09-17T21:25:36.494598Z","shell.execute_reply":"2023-09-17T21:25:38.874965Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Missing value handling","metadata":{}},{"cell_type":"code","source":"def fill_missing_values(df, categorical_columns, method='most_frequent'):\n    \"\"\"\n    Fill missing values in categorical columns of a DataFrame.\n\n    Args:\n        df (pandas.DataFrame): Input DataFrame.\n        categorical_columns (list): List of categorical column names to fill missing values.\n        method (str): Method for filling missing values. Options: 'most_frequent', 'unknown'.\n            Default is 'most_frequent'.\n\n    Returns:\n        pandas.DataFrame: DataFrame with missing values filled in categorical columns.\n    \"\"\"\n    df_filled = df.copy()\n    for column in categorical_columns:\n        if method == 'most_frequent':\n            df_filled[column].fillna(df_filled[column].mode().iloc[0], inplace=True)\n        elif method == 'unknown':\n            df_filled[column].fillna('Unknown', inplace=True)\n    \n    return df_filled\n","metadata":{"execution":{"iopub.status.busy":"2023-09-17T21:25:38.878637Z","iopub.execute_input":"2023-09-17T21:25:38.879154Z","iopub.status.idle":"2023-09-17T21:25:38.888160Z","shell.execute_reply.started":"2023-09-17T21:25:38.879110Z","shell.execute_reply":"2023-09-17T21:25:38.886813Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = fill_missing_values(df,df.columns)","metadata":{"execution":{"iopub.status.busy":"2023-09-17T21:25:38.889882Z","iopub.execute_input":"2023-09-17T21:25:38.890292Z","iopub.status.idle":"2023-09-17T21:25:40.390631Z","shell.execute_reply.started":"2023-09-17T21:25:38.890262Z","shell.execute_reply":"2023-09-17T21:25:40.389298Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Sampling","metadata":{}},{"cell_type":"code","source":"def reservoir_sampling(data, k):\n    sample = []\n\n    # Fill the sample array with the first k elements\n    for i in range(k):\n        sample.append(data.iloc[i])\n\n    # Replace elements in the sample with a decreasing probability\n    for i in range(k, len(data)):\n        j = random.randint(0, i)  # Generate a random index\n\n        if j < k:\n            sample[j] = data.iloc[i]  # Replace element at random index with new element from data\n\n    return pd.DataFrame(sample)\n","metadata":{"execution":{"iopub.status.busy":"2023-09-17T21:25:40.392874Z","iopub.execute_input":"2023-09-17T21:25:40.393271Z","iopub.status.idle":"2023-09-17T21:25:40.400298Z","shell.execute_reply.started":"2023-09-17T21:25:40.393240Z","shell.execute_reply":"2023-09-17T21:25:40.399159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_sample = reservoir_sampling(df,100000)","metadata":{"execution":{"iopub.status.busy":"2023-09-17T21:25:40.440739Z","iopub.execute_input":"2023-09-17T21:25:40.441719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Check the distribution of main data and sampling data","metadata":{}},{"cell_type":"code","source":"df_sample.hist(figsize=(15,10))\nplt.subplots_adjust(hspace=0.5)","metadata":{"execution":{"iopub.status.busy":"2023-06-03T09:26:29.713932Z","iopub.execute_input":"2023-06-03T09:26:29.714863Z","iopub.status.idle":"2023-06-03T09:26:32.610698Z","shell.execute_reply.started":"2023-06-03T09:26:29.714830Z","shell.execute_reply":"2023-06-03T09:26:32.609762Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.hist(figsize=(15,10))\nplt.subplots_adjust(hspace=0.5)","metadata":{"execution":{"iopub.status.busy":"2023-06-03T09:26:32.611890Z","iopub.execute_input":"2023-06-03T09:26:32.612638Z","iopub.status.idle":"2023-06-03T09:26:36.020234Z","shell.execute_reply.started":"2023-06-03T09:26:32.612608Z","shell.execute_reply":"2023-06-03T09:26:36.018894Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Hypothesis check","metadata":{}},{"cell_type":"markdown","source":" We want to be more confident aboat the distribution of data_sampler and main data","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom scipy import stats\n\ndef compare_distributions(data1, data2):\n    \"\"\"\n    Compare the distributions of two datasets using statistical tests.\n\n    Args:\n        data1 (array-like): First dataset.\n        data2 (array-like): Second dataset.\n\n    Returns:\n        bool: True if the distributions are similar, False otherwise.\n    \"\"\"\n    # Perform Kolmogorov-Smirnov test\n    ks_statistic, ks_pvalue = stats.ks_2samp(data1, data2)\n\n    # Perform Mann-Whitney U test\n    mw_u_statistic, mw_pvalue = stats.mannwhitneyu(data1, data2, alternative='two-sided')\n\n    # Set the significance level\n    alpha = 0.05\n\n    # Compare p-values to the significance level\n    if ks_pvalue > alpha and mw_pvalue > alpha:\n        return True  # Distributions are similar\n    else:\n        return False  # Distributions are different\n","metadata":{"execution":{"iopub.status.busy":"2023-09-17T21:23:36.056775Z","iopub.status.idle":"2023-09-17T21:23:36.057657Z","shell.execute_reply.started":"2023-09-17T21:23:36.057349Z","shell.execute_reply":"2023-09-17T21:23:36.057377Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in df.columns:\n    try:\n        print('Is dis of data_sample and data in col {} same :'.format(i),compare_distributions(df[i],df_sample[i]))\n    except:\n        continue","metadata":{"execution":{"iopub.status.busy":"2023-09-17T21:23:36.059245Z","iopub.status.idle":"2023-09-17T21:23:36.060094Z","shell.execute_reply.started":"2023-09-17T21:23:36.059795Z","shell.execute_reply":"2023-09-17T21:23:36.059823Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Engineering","metadata":{}},{"cell_type":"code","source":"\nencoder = LabelEncoder()\n\n# Encode the columns\ndf_sample['department'] = encoder.fit_transform(df_sample['department'])\ndf_sample['product_name'] = encoder.fit_transform(df_sample['product_name'])\n# df_sample['order_id'] = encoder.fit_transform(df_sample['order_id'])\n# df_sample['user_id'] = encoder.fit_transform(df_sample['user_id'])","metadata":{"execution":{"iopub.status.busy":"2023-09-17T21:27:33.566332Z","iopub.execute_input":"2023-09-17T21:27:33.566755Z","iopub.status.idle":"2023-09-17T21:27:33.651376Z","shell.execute_reply.started":"2023-09-17T21:27:33.566723Z","shell.execute_reply":"2023-09-17T21:27:33.650303Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Scaling","metadata":{}},{"cell_type":"code","source":"scaler = StandardScaler()\ndf_sample = scaler.fit_transform(df_sample)\n","metadata":{"execution":{"iopub.status.busy":"2023-09-17T21:27:35.754025Z","iopub.execute_input":"2023-09-17T21:27:35.754635Z","iopub.status.idle":"2023-09-17T21:27:35.784933Z","shell.execute_reply.started":"2023-09-17T21:27:35.754601Z","shell.execute_reply":"2023-09-17T21:27:35.783551Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Models","metadata":{}},{"cell_type":"markdown","source":"## Visualiztion of the models","metadata":{}},{"cell_type":"code","source":"\ndef tsne_umap_pca(df,labels):\n    # TSNE Reduction\n    X_embedded = TSNE(n_components=3, learning_rate='auto',\n                  init='random', perplexity=3).fit_transform(df)\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='3d')\n    ax.scatter(X_embedded[:, 0], X_embedded[:, 1],c=labels,cmap='jet')\n    plt.show()\n    \n    # PCA Reduction\n    pca = PCA(n_components=2)\n    X_pca = pca.fit_transform(df)\n\n    plt.scatter(X_pca[:, 0], X_pca[:, 1], c=labels, cmap='viridis')\n    plt.xlabel('Principal Component 1')\n    plt.ylabel('Principal Component 2')\n    plt.title('PCA Scatter Plot')\n\n    cbar = plt.colorbar()\n    cbar.set_label('Cluster')\n\n    plt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-06-03T09:27:15.142743Z","iopub.execute_input":"2023-06-03T09:27:15.143156Z","iopub.status.idle":"2023-06-03T09:27:15.151044Z","shell.execute_reply.started":"2023-06-03T09:27:15.143128Z","shell.execute_reply":"2023-06-03T09:27:15.149808Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## KMEANS","metadata":{}},{"cell_type":"code","source":"\nclusters = []\n\nfor i in range(1, 30):\n    km = KMeans(n_clusters=i).fit(df_sample)\n    clusters.append(km.inertia_)\n    \n","metadata":{"execution":{"iopub.status.busy":"2023-06-03T09:27:15.152532Z","iopub.execute_input":"2023-06-03T09:27:15.153141Z","iopub.status.idle":"2023-06-03T09:29:16.292458Z","shell.execute_reply.started":"2023-06-03T09:27:15.153112Z","shell.execute_reply":"2023-06-03T09:29:16.291394Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(12, 8))\nsns.lineplot(x=list(range(1, 30)), y=clusters, ax=ax)\nax.set_title('Searching for Elbow')\nax.set_xlabel('Clusters')\nax.set_ylabel('Inertia')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-06-03T09:29:16.293928Z","iopub.execute_input":"2023-06-03T09:29:16.294238Z","iopub.status.idle":"2023-06-03T09:29:16.652415Z","shell.execute_reply.started":"2023-06-03T09:29:16.294213Z","shell.execute_reply":"2023-06-03T09:29:16.651189Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pca = PCA(n_components=2)\ndf_pca = pca.fit_transform(df_sample)\n\nrange_n_clusters = [4,5,6,10,15,20,25]\n\nfor n_clusters in range_n_clusters:\n    fig, (ax1, ax2) = plt.subplots(1, 2)\n    fig.set_size_inches(18, 7)\n\n    ax1.set_xlim([-0.1, 1])\n\n    ax1.set_ylim([0, len(df_sample) + (n_clusters + 1) * 10])\n\n    clusterer = KMeans(n_clusters=n_clusters, random_state=10)\n    cluster_labels = clusterer.fit_predict(df_sample)\n\n    silhouette_avg = silhouette_score(df_sample, cluster_labels)\n    print(\"For n_clusters =\", n_clusters,\n          \"The average silhouette_score is :\", silhouette_avg)\n\n    sample_silhouette_values = silhouette_samples(df_sample, cluster_labels)\n\n    y_lower = 10\n    for i in range(n_clusters):\n        ith_cluster_silhouette_values = \\\n            sample_silhouette_values[cluster_labels == i]\n\n        ith_cluster_silhouette_values.sort()\n\n        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n        y_upper = y_lower + size_cluster_i\n\n        color = cm.nipy_spectral(float(i) / n_clusters)\n        ax1.fill_betweenx(np.arange(y_lower, y_upper),\n                          0, ith_cluster_silhouette_values,\n                          facecolor=color, edgecolor=color, alpha=0.7)\n\n        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n\n        y_lower = y_upper + 10  # 10 for the 0 samples\n\n    ax1.set_title(\"The silhouette plot for the various clusters.\")\n    ax1.set_xlabel(\"The silhouette coefficient values\")\n    ax1.set_ylabel(\"Cluster label\")\n\n    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n\n    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n\n    \n    colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)\n    ax2.scatter(df_pca[:, 0], df_pca[:, 1], marker='.', s=30, lw=0, alpha=0.7,\n                c=colors, edgecolor='k')\n\n    centers = clusterer.cluster_centers_\n\n    ax2.scatter(centers[:, 0], centers[:, 1], marker='o',\n                c=\"white\", alpha=1, s=200, edgecolor='k')\n\n    for i, c in enumerate(centers):\n        ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1,\n                    s=50, edgecolor='k')\n\n    ax2.set_title(\"The visualization of the clustered data.\")\n    ax2.set_xlabel(\"Feature space for the 1st feature\")\n    ax2.set_ylabel(\"Feature space for the 2nd feature\")\n\n    plt.suptitle((\"Silhouette analysis for KMeans clustering on sample data \"\n                  \"with n_clusters = %d\" % n_clusters),\n                 fontsize=14, fontweight='bold')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-06-03T09:29:16.653866Z","iopub.execute_input":"2023-06-03T09:29:16.654266Z","iopub.status.idle":"2023-06-03T09:56:52.621640Z","shell.execute_reply.started":"2023-06-03T09:29:16.654237Z","shell.execute_reply":"2023-06-03T09:56:52.619838Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Hirerachical Clustering","metadata":{}},{"cell_type":"markdown","source":"We need sample from df_sample because the algoramative method doesnt work with 100k data","metadata":{}},{"cell_type":"code","source":"df_ssample = reservoir_sampling(pd.DataFrame(df_sample),10000)","metadata":{"execution":{"iopub.status.busy":"2023-09-17T21:27:57.657813Z","iopub.execute_input":"2023-09-17T21:27:57.658333Z","iopub.status.idle":"2023-09-17T21:27:59.799803Z","shell.execute_reply.started":"2023-09-17T21:27:57.658299Z","shell.execute_reply":"2023-09-17T21:27:59.798612Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_ssample","metadata":{"execution":{"iopub.status.busy":"2023-06-03T10:10:22.387812Z","iopub.execute_input":"2023-06-03T10:10:22.388268Z","iopub.status.idle":"2023-06-03T10:10:22.412957Z","shell.execute_reply.started":"2023-06-03T10:10:22.388229Z","shell.execute_reply":"2023-06-03T10:10:22.410826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.cluster import AgglomerativeClustering \n\nagglom = AgglomerativeClustering(n_clusters=5, linkage='average').fit(df_ssample)\n\npca = PCA(n_components=3)\ndf_pca = pd.DataFrame(pca.fit_transform(df_ssample))\n\ndf_pca['Labels'] = agglom.labels_\nplt.figure(figsize=(12, 8))\nsns.scatterplot(data=df_pca, x=0, y=1, hue=\"Labels\")\nplt.title('Agglomerative with 5 Clusters')\nplt.show()\n\nplt.figure(figsize=(12, 8))\nsns.scatterplot(data=df_pca, x=1, y=2, hue=\"Labels\")\nplt.title('Agglomerative with 5 Clusters')\nplt.show()\n\nplt.figure(figsize=(12, 8))\nsns.scatterplot(data=df_pca, x=0, y=2, hue=\"Labels\")\nplt.title('Agglomerative with 5 Clusters')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-06-03T10:18:51.526147Z","iopub.execute_input":"2023-06-03T10:18:51.526595Z","iopub.status.idle":"2023-06-03T10:18:59.804352Z","shell.execute_reply.started":"2023-06-03T10:18:51.526563Z","shell.execute_reply":"2023-06-03T10:18:59.803280Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndist = distance_matrix(df_ssample, df_ssample)\nprint(dist)","metadata":{"execution":{"iopub.status.busy":"2023-06-03T10:20:41.964573Z","iopub.execute_input":"2023-06-03T10:20:41.965293Z","iopub.status.idle":"2023-06-03T10:20:47.157550Z","shell.execute_reply.started":"2023-06-03T10:20:41.965258Z","shell.execute_reply":"2023-06-03T10:20:47.156452Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Z = hierarchy.linkage(dist, 'complete')\n","metadata":{"execution":{"iopub.status.busy":"2023-06-03T10:20:47.159540Z","iopub.execute_input":"2023-06-03T10:20:47.159975Z","iopub.status.idle":"2023-06-03T10:26:40.949217Z","shell.execute_reply.started":"2023-06-03T10:20:47.159938Z","shell.execute_reply":"2023-06-03T10:26:40.947938Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(18, 50))\ndendro = hierarchy.dendrogram(Z, leaf_rotation=0, leaf_font_size=12, orientation='right')","metadata":{"execution":{"iopub.status.busy":"2023-06-03T10:26:40.950733Z","iopub.execute_input":"2023-06-03T10:26:40.951082Z","iopub.status.idle":"2023-06-03T10:28:02.099300Z","shell.execute_reply.started":"2023-06-03T10:26:40.951055Z","shell.execute_reply":"2023-06-03T10:28:02.098132Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# DBSCAN","metadata":{}},{"cell_type":"code","source":"# df_ssample = reservoir_sampling(df_sample,10000)","metadata":{"execution":{"iopub.status.busy":"2023-06-03T10:28:19.081597Z","iopub.execute_input":"2023-06-03T10:28:19.082063Z","iopub.status.idle":"2023-06-03T10:28:19.159445Z","shell.execute_reply.started":"2023-06-03T10:28:19.082028Z","shell.execute_reply":"2023-06-03T10:28:19.157972Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pca = PCA(n_components=2)\ndf_pca = pca.fit_transform(df_ssample)\nscaler = StandardScaler()\ndf_ssample = scaler.fit_transform(df_ssample)\n","metadata":{"execution":{"iopub.status.busy":"2023-09-17T21:27:50.827280Z","iopub.execute_input":"2023-09-17T21:27:50.827763Z","iopub.status.idle":"2023-09-17T21:27:50.890605Z","shell.execute_reply.started":"2023-09-17T21:27:50.827731Z","shell.execute_reply":"2023-09-17T21:27:50.888667Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"range_n_clusters = [5,15,30,45]\nrange_e = [1,2,3,4,5]\nfor eps in range_e:\n    for n_clusters in range_n_clusters:\n        try:\n            fig, (ax1, ax2) = plt.subplots(1, 2)\n            fig.set_size_inches(18, 7)\n\n            ax1.set_xlim([-0.1, 1])\n            ax1.set_ylim([0, len(df_ssample) + (n_clusters + 1) * 10])\n\n            clusterer = DBSCAN(eps=eps, min_samples=n_clusters)\n            cluster_labels = clusterer.fit_predict(df_ssample)\n\n            silhouette_avg = silhouette_score(df_ssample, cluster_labels)\n            print(\"For n_clusters =\", n_clusters,\n                  \"The average silhouette_score is :\", silhouette_avg)\n\n            sample_silhouette_values = silhouette_samples(df_ssample, cluster_labels)\n\n            y_lower = 10\n            for i in range(n_clusters):\n                ith_cluster_silhouette_values = \\\n                    sample_silhouette_values[cluster_labels == i]\n\n                ith_cluster_silhouette_values.sort()\n\n                size_cluster_i = ith_cluster_silhouette_values.shape[0]\n                y_upper = y_lower + size_cluster_i\n\n                color = cm.nipy_spectral(float(i) / n_clusters)\n                ax1.fill_betweenx(np.arange(y_lower, y_upper),\n                                  0, ith_cluster_silhouette_values,\n                                  facecolor=color, edgecolor=color, alpha=0.7)\n\n                ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n\n                y_lower = y_upper + 10  # 10 for the 0 samples\n\n            ax1.set_title(\"The silhouette plot for the various clusters.\")\n            ax1.set_xlabel(\"The silhouette coefficient values\")\n            ax1.set_ylabel(\"Cluster label\")\n\n            ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n\n            ax1.set_yticks([])  # Clear the yaxis labels / ticks\n            ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n\n            colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)\n            ax2.scatter(df_ssample[:, 0], df_ssample[:, 1], marker='.', s=30, lw=0, alpha=0.7,\n                        c=colors, edgecolor='k')\n\n            ax2.set_title(\"The visualization of the clustered data.\")\n            ax2.set_xlabel(\"Feature space for the 1st feature\")\n            ax2.set_ylabel(\"Feature space for the 2nd feature\")\n\n            plt.suptitle((\"Silhouette analysis for DSCAN clustering on sample data \"\n                          \"with n_clusters = %d\" % n_clusters),\n                         fontsize=14, fontweight='bold')\n        except:\n            continue\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-06-03T10:28:39.189895Z","iopub.execute_input":"2023-06-03T10:28:39.190333Z","iopub.status.idle":"2023-06-03T10:30:36.241543Z","shell.execute_reply.started":"2023-06-03T10:28:39.190301Z","shell.execute_reply":"2023-06-03T10:30:36.240674Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# HDBSCAN","metadata":{}},{"cell_type":"markdown","source":"Why HDBSCAN? HDBSCAN (Hierarchical Density-Based Spatial Clustering of Applications with Noise) is a powerful density-based clustering algorithm that offers several advantages in comparison to other clustering techniques. Here are some reasons why HDBSCAN is often preferred:\n\nRobust to parameter selection: HDBSCAN automatically determines the number of clusters based on the density and distribution of data points. This eliminates the need for users to specify the number of clusters beforehand, which can be challenging in many cases. HDBSCAN uses a hierarchical approach to identify clusters of different densities and allows for capturing both dense and sparse regions in the data.\n\nAbility to handle varying cluster densities and shapes: HDBSCAN can effectively handle clusters of different densities, including clusters with irregular shapes and varying densities within a single cluster. It utilizes the concept of core samples, which are data points that are densely connected, and it allows for the formation of clusters based on the density connectivity of these core samples.\n\nNoise detection and handling: HDBSCAN can identify and label noise or outliers as \"noise\" rather than forcing them into clusters. This is particularly valuable in scenarios where the dataset contains noisy or irrelevant data points that do not belong to any meaningful clusters.\n\nEfficiency and scalability: HDBSCAN employs efficient data structures, such as the kd-tree, to speed up the clustering process. It can handle large datasets efficiently and scale well with increasing data size, making it suitable for applications with a large number of data points.\n\nFlexibility in distance metrics: HDBSCAN can work with various distance metrics, enabling it to handle different types of data and problem domains. It allows users to define the proximity measure that is most appropriate for their data, whether it is numerical, categorical, or mixed.\n\nHierarchical clustering visualization: HDBSCAN produces a hierarchical representation of clusters, known as a dendrogram or cluster tree. This visualization provides insights into the hierarchy of clusters and allows users to explore different levels of cluster granularity.","metadata":{}},{"cell_type":"code","source":"%%time\n\nX_embedded = TSNE(n_components=3, learning_rate='auto',\n                  init='random', perplexity=3).fit_transform(df_ssample)","metadata":{"execution":{"iopub.status.busy":"2023-09-17T21:28:05.836341Z","iopub.execute_input":"2023-09-17T21:28:05.836811Z","iopub.status.idle":"2023-09-17T21:30:31.359048Z","shell.execute_reply.started":"2023-09-17T21:28:05.836756Z","shell.execute_reply":"2023-09-17T21:30:31.358031Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nclusterer_train = hdbscan.HDBSCAN(prediction_data=True).fit(df_ssample)\nu_train, counts_train = np.unique(clusterer_train.labels_, return_counts=True)\n\n# clusterer_test = hdbscan.HDBSCAN(prediction_data=True, min_cluster_size = 200 if DEBUG else 1000).fit(embedding_test)\n# u_test, counts_test = np.unique(clusterer_test.labels_, return_counts=True)\n\nprint(u_train)\nprint(counts_train)\n\n","metadata":{"execution":{"iopub.status.busy":"2023-09-17T21:30:31.364517Z","iopub.execute_input":"2023-09-17T21:30:31.368114Z","iopub.status.idle":"2023-09-17T21:30:36.602443Z","shell.execute_reply.started":"2023-09-17T21:30:31.368056Z","shell.execute_reply":"2023-09-17T21:30:36.601184Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(10, 8))\nplt.scatter(X_embedded[:, 0], X_embedded[:, 1], s=5, c=clusterer_train.labels_, edgecolors='none', cmap='jet');","metadata":{"execution":{"iopub.status.busy":"2023-09-17T21:30:36.604193Z","iopub.execute_input":"2023-09-17T21:30:36.604571Z","iopub.status.idle":"2023-09-17T21:30:37.079097Z","shell.execute_reply.started":"2023-09-17T21:30:36.604542Z","shell.execute_reply":"2023-09-17T21:30:37.077651Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# OPTICS","metadata":{}},{"cell_type":"markdown","source":"Why OPTICS? OPTICS (Ordering Points To Identify the Clustering Structure) is a density-based clustering algorithm that is often used as an alternative to other popular clustering algorithms like k-means or hierarchical clustering. OPTICS offers several advantages and can be beneficial in certain scenarios:\n\nDensity-based clustering: OPTICS is a density-based clustering algorithm, which means it can identify clusters of arbitrary shape and does not rely on predefined cluster centroids or assumptions about cluster geometry. It is effective in identifying clusters of varying densities, clusters with irregular shapes, and clusters embedded within larger clusters.\n\nRobust to parameter selection: OPTICS does not require the upfront specification of the number of clusters, unlike k-means or some hierarchical clustering algorithms. This can be advantageous when the optimal number of clusters is unknown or difficult to determine. OPTICS can provide a cluster ordering or reachability plot that allows users to explore different cluster granularity levels.\n\nCluster reachability analysis: OPTICS produces a reachability plot, which provides insights into the density-based structure of the data. The reachability plot reveals the varying density levels of the data points and can help in understanding the hierarchical relationships between clusters and detecting noise or outliers. This information is not readily available in many other clustering algorithms.\n\nFlexibility in distance metric: OPTICS can work with various distance metrics, allowing users to define the proximity measure that suits their data and problem domain. This flexibility enables the algorithm to handle different types of data, including numerical, categorical, or mixed data.\n\nHandling large datasets: OPTICS can handle large datasets efficiently by utilizing an indexing structure called the \"OPTICS ordering.\" This structure allows for incremental processing and avoids the need to calculate the entire pairwise distance matrix, making it scalable to datasets with a large number of data points.","metadata":{}},{"cell_type":"code","source":"df_ssample.shape","metadata":{"execution":{"iopub.status.busy":"2023-09-17T21:51:54.106460Z","iopub.execute_input":"2023-09-17T21:51:54.106928Z","iopub.status.idle":"2023-09-17T21:51:54.115748Z","shell.execute_reply.started":"2023-09-17T21:51:54.106897Z","shell.execute_reply":"2023-09-17T21:51:54.114235Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = df_sample","metadata":{"execution":{"iopub.status.busy":"2023-09-17T21:52:01.165526Z","iopub.execute_input":"2023-09-17T21:52:01.165995Z","iopub.status.idle":"2023-09-17T21:52:01.171503Z","shell.execute_reply.started":"2023-09-17T21:52:01.165962Z","shell.execute_reply":"2023-09-17T21:52:01.170586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nclust = OPTICS(min_samples=50, xi=0.05, min_cluster_size=0.05)\nclust.fit(X)\n","metadata":{"execution":{"iopub.status.busy":"2023-09-17T21:52:02.529377Z","iopub.execute_input":"2023-09-17T21:52:02.529879Z","iopub.status.idle":"2023-09-17T21:52:20.017280Z","shell.execute_reply.started":"2023-09-17T21:52:02.529844Z","shell.execute_reply":"2023-09-17T21:52:20.015852Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\nlabels_050 = cluster_optics_dbscan(\n    reachability=clust.reachability_,\n    core_distances=clust.core_distances_,\n    ordering=clust.ordering_,\n    eps=0.5,\n)\nlabels_200 = cluster_optics_dbscan(\n    reachability=clust.reachability_,\n    core_distances=clust.core_distances_,\n    ordering=clust.ordering_,\n    eps=2,\n)\n","metadata":{"execution":{"iopub.status.busy":"2023-09-17T22:07:49.619378Z","iopub.execute_input":"2023-09-17T22:07:49.620189Z","iopub.status.idle":"2023-09-17T22:07:49.629609Z","shell.execute_reply.started":"2023-09-17T22:07:49.620146Z","shell.execute_reply":"2023-09-17T22:07:49.628526Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.gridspec as gridspec\n\nspace = np.arange(len(X))\nreachability = clust.reachability_[clust.ordering_]\nlabels = clust.labels_[clust.ordering_]\n\nplt.figure(figsize=(10, 7))\nG = gridspec.GridSpec(2, 3)\nax1 = plt.subplot(G[0, :])\nax2 = plt.subplot(G[1, 0])\nax3 = plt.subplot(G[1, 1])\nax4 = plt.subplot(G[1, 2])\n\n# Reachability plot\ncolors = [\"g.\", \"r.\", \"b.\", \"y.\", \"c.\"]\nfor klass, color in zip(range(0, 5), colors):\n    Xk = space[labels == klass]\n    Rk = reachability[labels == klass]\n    ax1.plot(Xk, Rk, color, alpha=0.3)\nax1.plot(space[labels == -1], reachability[labels == -1], \"k.\", alpha=0.3)\nax1.plot(space, np.full_like(space, 2.0, dtype=float), \"k-\", alpha=0.5)\nax1.plot(space, np.full_like(space, 0.5, dtype=float), \"k-.\", alpha=0.5)\nax1.set_ylabel(\"Reachability (epsilon distance)\")\nax1.set_title(\"Reachability Plot\")\n\n\n","metadata":{"execution":{"iopub.status.busy":"2023-09-17T22:07:51.690429Z","iopub.execute_input":"2023-09-17T22:07:51.690939Z","iopub.status.idle":"2023-09-17T22:07:52.887805Z","shell.execute_reply.started":"2023-09-17T22:07:51.690902Z","shell.execute_reply":"2023-09-17T22:07:52.886619Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}